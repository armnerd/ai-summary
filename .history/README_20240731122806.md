# AI resource navigation

## 一.LangChain
+ [LangChain-tools](https://python.langchain.com/v0.2/docs/integrations/tools/)

## 二.Transformer

## 三.Run Models
### 3.1 [LMStudio](https://lmstudio.ai/)
```bash
1.本地下载大模型(选GGUF量化版本)
https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/tree/main

2.文件夹->Reveal in Finder
# 手工下载模型示例:
https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q2_k.gguf

# 创建目录结构和HF对照起来
mkdir -p Qwen/Qwen2-7B-Instruct-GGUF
cd Qwen/Qwen2-7B-Instruct-GGUF
 
# 国内可以使用如下镜像站点
wget https://hf-mirror.com/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q2_k.gguf
 
# 海外
wget https://huggingface.co/Qwen/Qwen2-7B-Instruct-GGUF/resolve/main/qwen2-7b-instruct-q2_k.gguf 
```

### 3.2 ollama

```bash

# 其实是一个本地模型的管理软件 里面有很多本地的大模型
https://ollama.com/library 
https://ollama.com/download 
https://ollama.com/library/llama3 
https://github.com/ollama/ollama/blob/main/docs/linux.md

# 免费的gpu
https://colab.google

# ollama核心参数
Ollama模型位置OLLAMA_MODELS   # 远程和本地都可以 修改大模型下载的位置 服务端
Ollama主机位置OLLAMA_HOST     # 本地客户端控制远程主机 主机为远程地址

安装说明：
// Server端
apt-get update -y
apt-get upgrade -y
apt install -y net-tools
curl -fsSL https://ollama.com/install.sh | sh
reboot

vim /etc/systemd/system/ollama.service

# ENV为新添加的特殊配置 目的是为了让本地Server端可以外部调用
Environment="OLLAMA_HOST=0.0.0.0:11434" # 指定监听地址,默认为127.0.0.1。
Environment="OLLAMA_ORIGINS=*""         # 指定允许跨域请求的源,外网测试 所以设置为*。
Environment="OLLAMA_MODELS=/data/ollama/models" # 型存放的路径,默认 ~/.ollama/models。
Environment="OLLAMA_MAX_LOADED_MODELS=4" # 加载最大模型数量
Environment="OLLAMA_NUM_PARALLEL=5"      # 设置最大并行请求数量，默认是1。



systemctl daemon-reload
systemctl restart ollama.service

// client
jesse@MacBook-Pro ~ % cat .zshrc
# 添加ollama server主机地址 以便于本地管理远程
export OLLAMA_HOST="101.237.38.36:11434"

source .zshrc


执行模型下载 可以看到 直接下载到了远端服务器：
ollama run qwen2:7b
ollama run llama3:8b

// 部署webui
open-webui: https://github.com/open-webui/open-webui 
https://docs.openwebui.com/
//需要提前安装好docker环境支持
https://docs.docker.com/engine/install/ubuntu/


docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://101.237.38.36:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

 
// 国内可用的镜像源
docker run -d -p 80:8080 -e OLLAMA_BASE_URL=http://127.0.0.1:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.chenby.cn/open-webui/open-webui:main


// ollama变量参数备注
Ollama可用环境变量汇总
OLLAMA_DEBUG
作用：显示额外的调试信息，例如设置为OLLAMA_DEBUG=1。

OLLAMA_FLASH_ATTENTION
作用：启用闪存注意力机制。

OLLAMA_HOST
作用：指定Ollama服务器的IP地址，默认是127.0.0.1:11434。

OLLAMA_KEEP_ALIVE
作用：模型在内存中保持加载的时间，默认是5分钟。

OLLAMA_LLM_LIBRARY
作用：设置LLM库以绕过自动检测。

OLLAMA_MAX_LOADED_MODELS
作用：设置最大加载模型数量，默认是1。

OLLAMA_MAX_QUEUE
作用：设置请求队列的最大数量。

OLLAMA_MAX_VRAM
作用：设置最大显存（VRAM）。

OLLAMA_MODELS
作用：指定模型目录的路径。

OLLAMA_NOHISTORY
作用：不保存Readline历史记录。

OLLAMA_NOPRUNE
作用：启动时不修剪模型数据。

OLLAMA_NUM_PARALLEL
作用：设置最大并行请求数量，默认是1。

OLLAMA_ORIGINS
作用：指定允许访问的源的逗号分隔列表。

OLLAMA_RUNNERS_DIR
作用：指定Runners的位置。

OLLAMA_SCHED_SPREAD
作用：总是将模型调度在所有GPU上。

OLLAMA_TMPDIR
作用：指定临时文件的位置。
```

### 3.3 AutoGen
```bash
1.地址
https://github.com/microsoft/autogen

2.M1安装
M1安装AutoGenstudio

# install
pip install --upgrade pip
pip install autogenstudio
   
# run
autogenstudio ui --host 0.0.0.0 --workers 10 --port 8080
   
# 安装python开发工具 pycharm，开发调试使用。
```

## 四.环境管理

### 4.1 miniconda管理Python环境

```bash
# 安装miniconda 管理PY环境
# https://docs.anaconda.com/miniconda/
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh

~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh

# 创建专属虚拟环境例子
conda create -n graphrag python=3.12 -y

# Mac Environment Location: 
/Users/jesse/miniconda3/envs/graphrag

# Activate this environment
conda activate graphrag

# Deactivate an active environment
conda deactivate

# 环境管理
conda env list
conda env remove -n env_name -y

# 其它命令
conda --version
conda env list
conda env -h

# conda升级
conda update conda    # 升级conda
conda update anaconda # 升级anaconda前要先升级conda
conda update --all    # 升级所有包

# 在升级完成之后,使用命令来清理一些无用的包释放一些空间。
conda clean -p # 删除没有用的包
conda clean -t # 删除保存下来的压缩文件（.tar）
```

## 五.RAG

### 5.1 GraphRAG(微软开源)
+ [Graph-based Retrieval Augmented Generation](https://github.com/microsoft/graphrag)
+ [GraphRAG-docs](https://microsoft.github.io/graphrag/)


## 六.镜像源
### 6.1 PIP镜像源
```bash
1.内地镜像源
清华：https://pypi.tuna.tsinghua.edu.cn/simple
阿里云：http://mirrors.aliyun.com/pypi/simple/
中国科技大学： https://pypi.mirrors.ustc.edu.cn/simple/
华中理工大学：http://pypi.hustunique.com/
山东理工大学：http://pypi.sdutlinux.org/ 
豆瓣：http://pypi.douban.com/simple/

2.临时用法示例:
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple graphRAG

3.永久修改:

# 方式一
vim ~/.pip/pip.conf
#------------------------------------------------------------ 
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
#------------------------------------------------------------ 

# 方式二
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

### 6.2 conda镜像源
+ [anaconda](https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/)

```bash
# 镜像源配置
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes

# 删除添加的源,恢复官方源
conda config --remove-key channels

# 查看源的优先权
XX@MacBook-Pro ~ % conda config --get channels
--add channels 'defaults'   # lowest priority
--add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'
--add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/'   # highest priority
```


+ [miniconda](https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/)

```bash
Miniconda是一个Anaconda的轻量级替代,默认只包含了python和conda,可以通过pip和conda  
来安装所需要的包,相关设置同上。
```


## 三十.AI
+ [AI-Tools](https://github.com/Tavely/Popular-AI-tools-list-by-category/blob/main/README_ZH.md)
